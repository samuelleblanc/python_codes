{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    \"\"\"\n",
    "       Collection of codes to load and analyze various data\n",
    "       \n",
    "           - modis \n",
    "           - emas \n",
    "           - cpl_layers text file\n",
    "           - apr2 files\n",
    "           - hdf files\n",
    "           \n",
    "        details are in the info of each module\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_modis(geofile,datfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_modis\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load Modis files\n",
    "        from within another script\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        modis,modis_dict = load_modis(geofile,datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        geofile name\n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        modis dictionary with tau, ref, etau, eref, phase, qa\n",
    "        modis_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        geo and dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-08, NASA Ames\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    modis_values = (#('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "          #          ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "           #         ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "            #        ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "                    ('cth',183)\n",
    "             #       ('cloud_mask',110)\n",
    "                    )\n",
    "    geosds = gdal.Open(geofile)\n",
    "    datsds = gdal.Open(datfile)\n",
    "    geosub = geosds.GetSubDatasets()\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    print('Outputting the Geo subdatasets:')\n",
    "    for i in range(len(geosub)):\n",
    "        print(str(i)+': '+geosub[i][1])\n",
    "    print('Outputting the Data subdatasets:')\n",
    "    for i in range(len(datsub)):\n",
    "        if any(i in val for val in modis_values):\n",
    "            print('\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1]))\n",
    "        else:\n",
    "            print(str(i)+': '+datsub[i][1])\n",
    "    latsds = gdal.Open(geosub[12][0],gdal.GA_ReadOnly)\n",
    "    lonsds = gdal.Open(geosub[13][0],gdal.GA_ReadOnly)\n",
    "    szasds = gdal.Open(geosub[21][0],gdal.GA_ReadOnly)\n",
    "    modis = dict()\n",
    "    modis['lat'] = latsds.ReadAsArray()\n",
    "    modis['lon'] = lonsds.ReadAsArray()\n",
    "    modis['sza'] = szasds.ReadAsArray()\n",
    "    print(modis['lon'].shape)\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    modis_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in modis_values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        modis_dicts[i] = sds.GetMetadata()\n",
    "        modis[i] = np.array(sds.ReadAsArray())\n",
    "        makenan = True\n",
    "        bad_points = np.where(modis[i] == float(modis_dicts[i]['_FillValue']))\n",
    "        scale = float(modis_dicts[i]['scale_factor'])\n",
    "        offset = float(modis_dicts[i]['add_offset'])\n",
    "       # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "        if scale.is_integer():\n",
    "            scale = int(scale)\n",
    "            makenan = False\n",
    "        if scale != 1 and offset == 0:\n",
    "            modis[i] = modis[i]*scale+offset\n",
    "        if makenan:\n",
    "            modis[i][bad_points] = np.nan\n",
    "        progress(float(tuple(i[0] for i in modis_values).index(i))/len(modis_values)*100.)\n",
    "    endprogress()\n",
    "    print(list(modis.keys()))\n",
    "    del geosds, datsds,sds,lonsds,latsds,geosub,datsub\n",
    "    return modis,modis_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_ict(fname,return_header=False,make_nan=True):\n",
    "    \"\"\"\n",
    "    Simple ict file loader\n",
    "    created specifically to load the files from the iwg1 on board the G1 during TCAP, may work with others...\n",
    "    inputs:\n",
    "       fname: filename with full path\n",
    "       return_header: (default set to False) if True, returns data, header in that form\n",
    "       make_nan: (default set to True) if True, the values defined in the header to be missing data, usually -999, is changed to NaNs\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    f = open(fname,'r')\n",
    "    lines = f.readlines()\n",
    "    first = lines[0]\n",
    "    sep = ','\n",
    "    try:\n",
    "        num2skip = int(first.strip().split(sep)[0])\n",
    "    except ValueError:\n",
    "        print('Seperation is set to a space')\n",
    "        sep = None\n",
    "        num2skip = int(first.strip().split(sep)[0])\n",
    "    header = lines[0:num2skip]\n",
    "    factor = list(map(float,header[10].strip().split(sep)))\n",
    "    missing = list(map(float,header[11].strip().split(sep)))\n",
    "    f.close()\n",
    "    if any([i!=1 for i in factor]):\n",
    "        print('Some Scaling factors are not equal to one, Please check the factors:')\n",
    "        print(factor)\n",
    "    def mktime(txt):\n",
    "        return datetime.strptime(txt,'%Y-%m-%d %H:%M:%S')\n",
    "    def utctime(seconds_utc):\n",
    "        return float(seconds_utc)/3600.\n",
    "    conv = {\"Date_Time\":mktime, \"UTC\":utctime, \"Start_UTC\":utctime, \"TIME_UTC\":utctime, \"UTC_mid\":utctime}\n",
    "    data = np.genfromtxt(fname,names=True,delimiter=sep,skip_header=num2skip-1,converters=conv)\n",
    "    print(data.dtype.names)\n",
    "    #scale the values by using the scale factors\n",
    "    for i,name in enumerate(data.dtype.names):\n",
    "        if i>0:\n",
    "            if factor[i-1]!=float(1):\n",
    "                data[name] = data[name]*factor[i-1]\n",
    "            if make_nan:\n",
    "                data[name][data[name]==missing[i-1]]=np.NaN\n",
    "    if return_header:\n",
    "        return data, header\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:32:17.264284Z",
     "start_time": "2020-11-20T19:32:17.250371Z"
    },
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def modis_qa_MOD06(qa_array):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    \n",
    "        modis qa data parser for Cloud properties.\n",
    "\n",
    "            Set for useful COD and REF\n",
    "            Set for 2nd highest (not highest) confidence level\n",
    "            \n",
    "    Input: \n",
    "    \n",
    "        input of qa numpy array from MODIS MOD06/MYD06 hdf files\n",
    "        qa_array: 3d array of the QA values 'Quality_Assurance_1km' from MOD06 hdf, read by pyhdf.SD\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "        output qa numpy boolean array (True is high QA)\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        load_utils.bits_stripping\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        none\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        > from pyhdf.SD import SD, SDC\n",
    "        > fh = SD(str(fp+'data_other/MODIS/MOD06_L2/2016/264/MOD06_L2.A2016264.1030.061.2017328080629.hdf'),SDC.READ)\n",
    "        > co = fh.select('Cloud_Optical_Thickness')\n",
    "        > codm = co.get()\n",
    "        > codm = codm*co.attributes()['scale_factor']\n",
    "        > codm[codm<0] = np.nan\n",
    "        > qal = fh.select('Quality_Assurance_1km')\n",
    "        > qa = modis_qa_MOD06(qal.get())\n",
    "        > codm[~qa] = np.nan\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2020-11-23, Santa Cruz, CA, ported from earlier implementation in ORACLES_build_DARE\n",
    "        \n",
    "    \"\"\"\n",
    "    #bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
    "    from load_utils import bits_stripping\n",
    "    \n",
    "    # for clouds\n",
    "    qa_cod_useful = bits_stripping(0,1,qa_array[:,:,4])\n",
    "    qa_cod_conf = bits_stripping(1,2,qa_array[:,:,4])\n",
    "    qa_ref_useful = bits_stripping(3,1,qa_array[:,:,4])\n",
    "    qa_ref_conf = bits_stripping(4,2,qa_array[:,:,4])\n",
    "    qa = (qa_cod_useful>0) & (qa_cod_conf>1) & (qa_ref_useful>0) & (qa_ref_conf>1) \n",
    "    \n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T19:32:18.422100Z",
     "start_time": "2020-11-20T19:32:18.415223Z"
    }
   },
   "outputs": [],
   "source": [
    "def bits_stripping(bit_start,bit_count,value):\n",
    "    \"Support function to the modis_qa flags (MOD06) to parse out a bit array from hdf files\"\n",
    "    bitmask=pow(2,bit_start+bit_count)-1\n",
    "    return np.right_shift(np.bitwise_and(value,bitmask),bit_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mod06(fh,set_qa_nan=False):\n",
    "    \"\"\"\n",
    "    Purpose:\n",
    "    \n",
    "        Simple modis reader function, with input 'fh' is a SD object. \n",
    "        Part of broader analysis which preloads the hdf files for faster parallel analysis.\n",
    "        \n",
    "    Input: \n",
    "    \n",
    "        fh: file SD handle for Hdf MOD06/MYD06 files\n",
    "        set_qa_nan: if set (default False) will nan ou bad cloud QA values\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "        codm : Cloud optical thickness at 1km  \n",
    "        ref : cloud effective radius at 1km \n",
    "        \n",
    "    Dependencies:\n",
    "\n",
    "        load_utils.modis_qa_MOD06\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        MODIS hdf files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        > from pyhdf.SD import SD, SDC\n",
    "        > geosy[ad]['fh'] = [SD(str(fp+'data_other/MODIS/MYD06_L2/2016/264/MYD06_L2.A2016264.1315.061.2018062110512.hdf'),SDC.READ)]\n",
    "        > for igy,gy in enumerate(geosy[ad]['fh']):\n",
    "                ind = mu.map_ind(geosy[ad]['lat'][igy,:,:],geosy[ad]['lon'][igy,:,:],\n",
    "                         ar['Latitude'][fla][iaes][idd],ar['Longitude'][fla][iaes][idd])\n",
    "        >  if np.array(ind).any():\n",
    "                cody,refy = read_mod06(gy)\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2020-11-23, Santa Cruz, CA, ported from earlier implementation in ORACLES_build_DARE\n",
    "    \n",
    "    \"\"\"\n",
    "    from load_utils import modis_qa_MOD06\n",
    "    re = fh.select('Cloud_Effective_Radius')\n",
    "    ref = re.get()\n",
    "    ref = ref*re.attributes()['scale_factor']\n",
    "    ref[ref<0] = np.nan\n",
    "    \n",
    "    co = fh.select('Cloud_Optical_Thickness')\n",
    "    codm = co.get()\n",
    "    codm = codm*co.attributes()['scale_factor']\n",
    "    codm[codm<0] = np.nan\n",
    "        \n",
    "    qal = fh.select('Quality_Assurance_1km')\n",
    "    qa = modis_qa_MOD06(qal.get())\n",
    "    \n",
    "    if set_qa_nan:\n",
    "        codm[~qa] = np.nan\n",
    "        ref[~qa] = np.nan\n",
    "    \n",
    "    return codm,ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def mat2py_time(matlab_datenum):\n",
    "    \"convert a matlab datenum to a python datetime object. Works on numpy arrays of datenum\"\n",
    "    from datetime import datetime, timedelta\n",
    "    #matlab_datenum = 731965.04835648148\n",
    "    m2ptime = lambda tmat: datetime.fromordinal(int(tmat)) + timedelta(days=tmat%1) - timedelta(days = 366)\n",
    "    try:\n",
    "        python_datetime = m2ptim2(matlab_datenum)\n",
    "    except:\n",
    "        import numpy as np\n",
    "        python_datetime = np.array([m2ptime(matlab_datenum.flatten()[i]) for i in xrange(matlab_datenum.size)])\n",
    "    return python_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def toutc(pydatetime):\n",
    "    \"Convert python datetime to utc fractional hours\"\n",
    "    utc_fx = lambda x: float(x.hour)+float(x.minute)/60.0+float(x.second)/3600.0+float(x.microsecond)/3600000000.0\n",
    "    try: \n",
    "        return utc_fx(pydatetime)\n",
    "    except:\n",
    "        import numpy as np\n",
    "        try:\n",
    "            return np.array([utc_fx(pydatetime.flatten()[i])+(pydatetime.flatten()[i].day-pydatetime.flatten()[0].day)*24.0 \\\n",
    "                         for i in xrange(pydatetime.size)]) \n",
    "        except AttributeError:\n",
    "            return np.array([utc_fx(pydatetime[i])+(pydatetime[i].day-pydatetime[0].day)*24.0 \\\n",
    "                         for i in xrange(len(pydatetime))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_emas(datfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_emas\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load emas files\n",
    "        from within another script.\n",
    "        Similar to load_modis\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        emas,emas_dict = load_emas(datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        emas dictionary with tau, ref, etau, eref, phase, qa\n",
    "        emas_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-08, NASA Ames\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    emas_values = (#('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "          #          ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "           #         ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "            #        ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "             #       ('cloud_mask',110)\n",
    "                    )\n",
    "    datsds = gdal.Open(datfile)\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    print('Outputting the Data subdatasets:')\n",
    "    for i in range(len(datsub)):\n",
    "        if any(i in val for val in emas_values):\n",
    "            print('\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1]))\n",
    "        else:\n",
    "            print(str(i)+': '+datsub[i][1])\n",
    "    emas = dict()\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    emas_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in emas_values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        emas_dicts[i] = sds.GetMetadata()\n",
    "        emas[i] = np.array(sds.ReadAsArray())\n",
    "        makenan = True\n",
    "        bad_points = np.where(emas[i] == float(emas_dicts[i]['_FillValue']))\n",
    "        try:\n",
    "            scale = float(emas_dicts[i]['scale_factor'])\n",
    "            offset = float(emas_dicts[i]['add_offset'])\n",
    "            # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "            if scale.is_integer():\n",
    "               scale = int(scale)\n",
    "               makenan = False\n",
    "            if scale != 1 and offset == 0:\n",
    "               emas[i] = emas[i]*scale+offset\n",
    "        except:\n",
    "            if issubclass(emas[i].dtype.type, np.integer):\n",
    "                makenan = False\n",
    "        if makenan:\n",
    "            emas[i][bad_points] = np.nan\n",
    "        progress(float(tuple(i[0] for i in emas_values).index(i))/len(emas_values)*100.)\n",
    "    endprogress()\n",
    "    print(list(emas.keys()))\n",
    "    del datsds,sds,datsub\n",
    "    return emas,emas_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_hdf(datfile,values=None,verbose=True,all_values=False):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load emas files\n",
    "        from within another script.\n",
    "        Similar to load_modis\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        hdf_dat,hdf_dict = load_hdf(datfile,Values=None,verbose=True,all_values=False) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        hdf_dat dictionary with the names of values saved, with associated dictionary values\n",
    "        hdf_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        values: if ommitted, only outputs the names of the variables in file\n",
    "                needs to be a tuple of 2 element tuples (first element name of variable, second number of record)\n",
    "                example: modis_values=(('cloud_top',57),('phase',53),('cloud_top_temp',58),('ref',66),('tau',72))\n",
    "        verbose: if true (default), then everything is printed. if false, nothing is printed\n",
    "        all_values: if True, then outputs all the values with their original names, (defaults to False), overrides values keyword\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "        Sp_parameters for progress issuer\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-10, NASA Ames\n",
    "        Modified (v1.1): Samuel LeBlanc, 2015-04-10, NASA Ames\n",
    "                        - added verbose keyword\n",
    "        Modified (v1.2): Samuel LeBlanc, 2016-05-07, Osan AFB, Korea\n",
    "                        - added error handling for missing fill value\n",
    "        Modified (v1.3): Samuel LeBlanc, 2016-11-15, NASA Ames\n",
    "                        - added all_values keyword\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    from load_utils import load_hdf_h5py\n",
    "    \n",
    "    try:\n",
    "        datsds = gdal.Open(datfile)\n",
    "    except:\n",
    "        print('Trying with load_hdf_h5py')\n",
    "        return load_hdf_h5py(datfile,values=values,verbose=verbose,all_values=all_values)\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    if verbose: \n",
    "        print('Outputting the Data subdatasets:')\n",
    "        for i in range(len(datsub)):\n",
    "            if values:\n",
    "                if any(i in val for val in values):\n",
    "                    print('\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1]))\n",
    "                else:\n",
    "                    print(str(i)+': '+datsub[i][1])\n",
    "            else:\n",
    "                print(str(i)+': '+datsub[i][1])\n",
    "    if all_values:\n",
    "        values = []\n",
    "        for i in range(len(datsub)):\n",
    "            values.append((datsub[i][1].split(' ')[1],i))\n",
    "        values = tuple(values)\n",
    "    if not values:\n",
    "        if verbose:\n",
    "            print('Done going through file... Please supply pairs of name and index for reading file')\n",
    "            print(\" in format values = (('name1',index1),('name2',index2),('name3',index3),...)\")\n",
    "            print(\" where namei is the nameof the returned variable, and indexi is the index of the variable (from above)\")\n",
    "        return None, None\n",
    "    hdf = dict()\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    hdf_dicts = dict()\n",
    "    if verbose:\n",
    "        startprogress('Running through data values')\n",
    "    for i,j in values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        hdf_dicts[i] = sds.GetMetadata()\n",
    "        hdf[i] = np.array(sds.ReadAsArray())\n",
    "        if not hdf[i].any():\n",
    "            import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            bad_points = np.where(hdf[i] == float(hdf_dicts[i]['_FillValue']))\n",
    "            makenan = True\n",
    "        except KeyError:\n",
    "            makenan = False\n",
    "        except ValueError:\n",
    "            makenan = False\n",
    "            print('*** FillValue not used to replace NANs, will have to do manually ***')\n",
    "        try:\n",
    "            scale = float(hdf_dicts[i]['scale_factor'])\n",
    "            offset = float(hdf_dicts[i]['add_offset'])\n",
    "            # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "            if scale.is_integer():\n",
    "               scale = int(scale)\n",
    "               makenan = False\n",
    "            if scale != 1 and offset == 0:\n",
    "               hdf[i] = hdf[i]*scale+offset\n",
    "        except:\n",
    "            if issubclass(hdf[i].dtype.type, np.integer):\n",
    "                makenan = False\n",
    "        if makenan:\n",
    "            hdf[i][bad_points] = np.nan\n",
    "        if verbose:\n",
    "            progress(float(tuple(i[0] for i in values).index(i))/len(values)*100.)\n",
    "    if verbose:\n",
    "        endprogress()\n",
    "        print(list(hdf.keys()))\n",
    "    del datsds,sds,datsub\n",
    "    return hdf,hdf_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf_h5py(datfile,values=None,verbose=True,all_values=False):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf_h5py\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load hdf5 files, through h5py python modules. \n",
    "        from within another script.\n",
    "        Similar to load_modis\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        hdf_dat,hdf_dict = load_hdf_h5py(datfile,Values=None,verbose=True,all_values=False) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        hdf_dat dictionary with the names of values saved, with associated dictionary values\n",
    "        hdf_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        values: if ommitted, only outputs the names of the variables in file\n",
    "                needs to be a tuple of 2 element tuples (first element name of variable, second number of record)\n",
    "                example: modis_values=(('cloud_top',57),('phase',53),('cloud_top_temp',58),('ref',66),('tau',72))\n",
    "        verbose: if true (default), then everything is printed. if false, nothing is printed\n",
    "        all_values: if True, then outputs all the values with their original names, (defaults to False), overrides values keyword\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "        Sp_parameters for progress issuer\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2020-06-26, Santa Cruz, CA, COVID-19 Quarantining\n",
    "        Modified (v1.1): \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import h5py\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    \n",
    "    dat = h5py.File(datfile,'r')\n",
    "    if verbose: print('Reading file: {}'.format(datfile))\n",
    "    datsub = list(dat.items())\n",
    "    if verbose: \n",
    "        print('Outputting the Data subdatasets:')\n",
    "        for i in range(len(datsub)):\n",
    "            if values:\n",
    "                if any(i in val for val in values):\n",
    "                    print('\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][0]))\n",
    "                else:\n",
    "                    print(str(i)+': '+datsub[i][0])\n",
    "            else:\n",
    "                print(str(i)+': '+datsub[i][0])\n",
    "    if all_values:\n",
    "        values = []\n",
    "        for i in range(len(datsub)):\n",
    "            values.append((datsub[i][0],i))\n",
    "        values = tuple(values)\n",
    "    if not values:\n",
    "        if verbose:\n",
    "            print('Done going through file... Please supply pairs of name and index for reading file')\n",
    "            print(\" in format values = (('name1',index1),('name2',index2),('name3',index3),...)\")\n",
    "            print(\" where namei is the nameof the returned variable, and indexi is the index of the variable (from above)\")\n",
    "        return None, None\n",
    "    hdf = dict()\n",
    "    meta = dict(dat.attrs)\n",
    "    import gc; gc.collect()\n",
    "    hdf_dicts = dict()\n",
    "    if verbose:\n",
    "        startprogress('Running through data values')\n",
    "    for i,j in values:\n",
    "        sds = dat[i]\n",
    "        hdf_dicts[i] = dict(sds.attrs)\n",
    "        hdf[i] = np.array(dat[i].value)\n",
    "        if not hdf[i].any():\n",
    "            import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            bad_points = np.where(hdf[i] == sds.fillvalue)\n",
    "            makenan = True\n",
    "        except KeyError:\n",
    "            makenan = False\n",
    "        except ValueError:\n",
    "            makenan = False\n",
    "            print('*** FillValue not used to replace NANs, will have to do manually ***')\n",
    "        try:\n",
    "            scale = float(hdf_dicts[i]['scale_factor'])\n",
    "            offset = float(hdf_dicts[i]['add_offset'])\n",
    "            # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "            if scale.is_integer():\n",
    "               scale = int(scale)\n",
    "               makenan = False\n",
    "            if scale != 1 and offset == 0:\n",
    "               hdf[i] = hdf[i]*scale+offset\n",
    "        except:\n",
    "            if issubclass(hdf[i].dtype.type, np.integer):\n",
    "                makenan = False\n",
    "        if makenan:\n",
    "            hdf[i][bad_points] = np.nan\n",
    "        if verbose:\n",
    "            progress(float(tuple(i[0] for i in values).index(i))/len(values)*100.)\n",
    "    if verbose:\n",
    "        endprogress()\n",
    "        print(list(hdf.keys()))\n",
    "    hdf_dicts['Global'] = meta\n",
    "    del sds,datsub\n",
    "    return hdf,hdf_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_cpl_layers(datfile,values=None):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_cpl_layers\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Function to load cpl files of layer properties 'layers_'\n",
    "        This funciton is called from another script\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        cpl_layers = load_cpl_layers(datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (layers text files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        cpl_layers: dictionary with associated properties such as time, A/C altitude, latitude, longitude, and roll\n",
    "                    number of layers, Ground height (GH) in meters above MSL, top and bottom altitude of each layer, and type of layer\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        os\n",
    "        numpy\n",
    "        re : for regular experessions\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        layers text files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-03-24, NASA Ames\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not(os.path.isfile(datfile)):\n",
    "        error('File not found!')\n",
    "    import numpy as np\n",
    "    import re\n",
    "\n",
    "    # set variables to read\n",
    "    num_lines = sum(1 for line in open(datfile))\n",
    "    head_lines = 14\n",
    "    d = np.empty(num_lines-head_lines,dtype=[('hh','i4'),\n",
    "                                             ('mm','i4'),\n",
    "                                             ('ss','i4'),\n",
    "                                             ('lat','f8'),\n",
    "                                             ('lon','f8'),\n",
    "                                             ('alt','f8'),\n",
    "                                             ('rol','f8'),\n",
    "                                             ('num','i4'),\n",
    "                                             ('gh','f8'),\n",
    "                                             ('top','f8',(10)),\n",
    "                                             ('bot','f8',(10)),\n",
    "                                             ('type','i4',(10)),\n",
    "                                             ('utc','f8')])\n",
    "    d[:] = np.NAN\n",
    "    header = ''\n",
    "    with open(datfile) as fp:\n",
    "        for iline, line in enumerate(fp):\n",
    "            if iline<head_lines:\n",
    "                header = header + line\n",
    "            else:\n",
    "                i = iline-head_lines\n",
    "                line = line.strip()\n",
    "                temp = [_f for _f in re.split(r\"[ :()]\",line) if _f]\n",
    "                d['hh'][i], d['mm'][i], d['ss'][i] = temp[0], temp[1], temp[2]\n",
    "                d['lat'][i], d['lon'][i], d['alt'][i] = temp[3], temp[4], temp[5]\n",
    "                d['rol'][i], d['num'][i], d['gh'][i] = temp[6], temp[7], temp[8]\n",
    "                for n in range(d['num'][i]):\n",
    "                    try:\n",
    "                        d['top'][i][n], d['bot'][i][n], d['type'][i][n] = temp[9+3*n], temp[9+3*n+1], temp[9+3*n+2]\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "                d['utc'][i] = float(d['hh'][i])+float(d['mm'][i])/60.0+float(d['ss'][i])/3600.0\n",
    "                \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-12T19:16:45.755921Z",
     "start_time": "2021-05-12T19:16:45.616504Z"
    },
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_apr(datfiles):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_apr\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Function to load apr values of zenith dbz from the various files in the datfiles list\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aprout = load_apr(datfiles) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfiles name (list of .h4 files to combine)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aprout: dbz, zenith radar reflectivity\n",
    "                latz, latitude of the zenith reflectivity\n",
    "                lonz, longitude of the \"\n",
    "                altflt, actual altitude in the atmosphere of the radar refl.\n",
    "                utc, time of measurement in utc fractional hours\n",
    "                \n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        os\n",
    "        numpy\n",
    "        load_modis\n",
    "        datetime\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        hdf APR-2 files from SEAC4RS\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-04-10, NASA Ames\n",
    "        Modified (v1.1): Samuel LeBlanc, 2016-11-15, NASA Ames\n",
    "                         - added Ka band (35 Ghz) reading from the file\n",
    "                         - and zenith dbz \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from load_utils import load_hdf\n",
    "    import datetime\n",
    "    \n",
    "    first = True\n",
    "    for f in datfiles:\n",
    "        print('Running file: ',f)\n",
    "        if not(os.path.isfile(f)):\n",
    "            print('Problem with file:', f)\n",
    "            print(' ... Skipping')\n",
    "            continue\n",
    "        \n",
    "        apr_value = (('lat',16),('lon',17),('alt',15),('time',13),('dbz',0),('lat3d',30),\n",
    "                     ('lon3d',31),('alt3d',32),('lat3d_o',24),('lon3d_o',25),('alt3d_o',26),\n",
    "                     ('lat3d_s',27),('lon3d_s',28),('alt3d_s',29),('dbz_35',3),('zen_dbz',4),\n",
    "                     ('alt3dz',41),('alt3dz_o',35),('alt3dz_s',38))\n",
    "        apr,aprdicts = load_hdf(f,values=apr_value,verbose=False)\n",
    "        # transform the 3d latitudes, longitudes, and altitudes to usable values\n",
    "        apr['latz'] = apr['lat3d']/apr['lat3d_s']+apr['lat3d_o']\n",
    "        apr['lonz'] = apr['lon3d']/apr['lon3d_s']+apr['lon3d_o']\n",
    "        apr['altz'] = apr['alt3d']/apr['alt3d_s']+apr['alt3d_o']\n",
    "        apr['altzen'] = apr['alt3dz']/apr['alt3dz_s']+apr['alt3dz_o']\n",
    "        apr['altflt'] = np.copy(apr['altz'])\n",
    "        apr['altfltz'] = np.copy(apr['altzen'])\n",
    "        try:\n",
    "            for z in range(apr['altz'].shape[0]):\n",
    "                apr['altflt'][z,:,:] = apr['altz'][z,:,:]+apr['alt'][z,:]\n",
    "            for z in range(apr['altzen'].shape[0]):\n",
    "                apr['altfltz'][z,:,:] = apr['altzen'][z,:,:]+apr['alt'][z,:]\n",
    "        except IndexError:\n",
    "            try:\n",
    "                print('swaping axes')\n",
    "                apr['altflt'] = np.swapaxes(apr['altflt'],0,1)\n",
    "                apr['altz'] = np.swapaxes(apr['altz'],0,1)\n",
    "                apr['altfltz'] = np.swapaxes(apr['altfltz'],0,1)\n",
    "                apr['altzen'] = np.swapaxes(apr['altzen'],0,1)\n",
    "                apr['latz'] = np.swapaxes(apr['latz'],0,1)\n",
    "                apr['lonz'] = np.swapaxes(apr['lonz'],0,1)\n",
    "                apr['dbz'] = np.swapaxes(apr['dbz'],0,1)\n",
    "                apr['dbz_35'] = np.swapaxes(apr['dbz_35'],0,1)\n",
    "                apr['zen_dbz'] = np.swapaxes(apr['zen_dbz'],0,1)\n",
    "                for z in range(apr['altz'].shape[0]):\n",
    "                    apr['altflt'][z,:,:] = apr['altz'][z,:,:]+apr['alt'][z,:]\n",
    "                for z in range(apr['altzen'].shape[0]):\n",
    "                    apr['altfltz'][z,:,:] = apr['altzen'][z,:,:]+apr['alt'][z,:]\n",
    "            except:\n",
    "                print('Problem file:',f)\n",
    "                print('... Skipping')\n",
    "                continue\n",
    "        except:\n",
    "            print('Problem with file: ',f)\n",
    "            print(' ... Skipping')\n",
    "            continue\n",
    "        izen = apr['altz'][:,0,0].argmax() #get the index of zenith\n",
    "        izenz = apr['altzen'][:,0,0].argmax() #get the index of zenith\n",
    "        if first:\n",
    "            aprout = dict()\n",
    "            aprout['dbz'] = apr['dbz'][izen,:,:]\n",
    "            aprout['dbz_35'] = apr['dbz_35'][izen,:,:]\n",
    "            aprout['zen_dbz'] = apr['zen_dbz'][izenz,:,:]\n",
    "            aprout['altflt'] = apr['altz'][izen,:,:]+apr['alt'][izen,:]\n",
    "            aprout['altfltz'] = apr['altzen'][izenz,:,:]+apr['alt'][izen,:]\n",
    "            aprout['latz'] = apr['latz'][izen,:,:]\n",
    "            aprout['lonz'] = apr['lonz'][izen,:,:]\n",
    "            v = datetime.datetime.utcfromtimestamp(apr['time'][izen,0])\n",
    "            aprout['utc'] = (apr['time'][izen,:]-(datetime.datetime(v.year,v.month,v.day,0,0,0)-datetime.datetime(1970,1,1)).total_seconds())/3600.\n",
    "            first = False\n",
    "        else:\n",
    "            aprout['dbz'] = np.concatenate((aprout['dbz'].T,apr['dbz'][izen,:,:].T)).T\n",
    "            aprout['dbz_35'] = np.concatenate((aprout['dbz_35'].T,apr['dbz'][izen,:,:].T)).T\n",
    "            aprout['zen_dbz'] = np.concatenate((aprout['zen_dbz'].T,apr['zen_dbz'][izenz,:,:].T)).T\n",
    "            aprout['altflt'] = np.concatenate((aprout['altflt'].T,(apr['altz'][izen,:,:]+apr['alt'][izen,:]).T)).T\n",
    "            aprout['altfltz'] = np.concatenate((aprout['altfltz'].T,(apr['altzen'][izenz,:,:]+apr['alt'][izen,:]).T)).T\n",
    "            aprout['latz'] = np.concatenate((aprout['latz'].T,apr['latz'][izen,:,:].T)).T\n",
    "            aprout['lonz'] = np.concatenate((aprout['lonz'].T,apr['lonz'][izen,:,:].T)).T\n",
    "            v = datetime.datetime.utcfromtimestamp(apr['time'][izen,0])\n",
    "            utc = (apr['time'][izen,:]-(datetime.datetime(v.year,v.month,v.day,0,0,0)-datetime.datetime(1970,1,1)).total_seconds())/3600.\n",
    "            aprout['utc'] = np.concatenate((aprout['utc'].T,utc.T)).T\n",
    "            \n",
    "    print(list(aprout.keys()))\n",
    "    print('Loaded data points: ', aprout['utc'].shape)\n",
    "    return aprout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_amsr(datfile,lonlatfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_amsr\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to load amsr data into a sucinct dictionary\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        amsr = load_amsr(datfile,lonlatfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile: path and name of hdf file\n",
    "        lonlatfile: path and name of hdf files for lat and lon\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        amsr: dictionary with numpy array of values\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "        Sp_parameters for progress issuer\n",
    "        pdb: for debugging\n",
    "        load_modis: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat file\n",
    "        lonlat file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-05-04, NASA Ames\n",
    "        \n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not(os.path.isfile(datfile)):\n",
    "        error('Data file not found!')\n",
    "    if not(os.path.isfile(lonlatfile)):\n",
    "        error('Lonlat file not found!')\n",
    "    import numpy as np\n",
    "    from load_utils import load_hdf\n",
    "    from osgeo import gdal\n",
    "    gdat = gdal.Open(datfile)\n",
    "    dat = dict()\n",
    "    dat['nfo'] = gdat.GetMetadata()\n",
    "    dat['ice'] = gdat.GetRasterBand(1).ReadAsArray()\n",
    "    datll,dicll = load_hdf(lonlatfile,values=(('lon',0),('lat',1)),verbose=False)\n",
    "    dat['lat'] = datll['lat']\n",
    "    dat['lon'] = datll['lon']\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_hdf_sd(FILE_NAME):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf_sd\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to load everyything in a hdf file using the SD protocol instead of GDAL\n",
    "        makes nans out of Missing_value and _FilValue. Scales the values by the scale_factor\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        dat,dat_dict = load_hdf_sd(FILE_NAME) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        FILE_NAME: path and name of hdf file\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        dat: dictionary with numpy array of values\n",
    "        dat_dict: dictionary with dictionaries of attributes for each read value\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        pyhdf, SDC, SD\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-05-13, NASA Ames\n",
    "        Modified (v1.1): by Samuel LeBlanc, 2015-07-01, NASA Ames, Happy Canada Day!\n",
    "                        - added Fill value keyword selection\n",
    "                        - added scale factor and add offset\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from pyhdf.SD import SD, SDC\n",
    "    print('Reading file: '+FILE_NAME)\n",
    "    hdf = SD(str(FILE_NAME), SDC.READ)\n",
    "    dat = dict()\n",
    "    dat_dict = dict()\n",
    "    for name in list(hdf.datasets().keys()):\n",
    "        print('  '+name+': %s' % (hdf.datasets()[name],))\n",
    "        dat[name] = hdf.select(name)[:]\n",
    "        dat_dict[name] = hdf.select(name).attributes()\n",
    "        try:\n",
    "            scale_factor = dat_dict[name].get('scale_factor')\n",
    "            if not scale_factor:\n",
    "                scale_factor = 1.0\n",
    "            dat[name] = dat[name]*scale_factor\n",
    "            try:\n",
    "                dat[name][dat[name] == dat_dict[name].get('missing_value')*scale_factor] = np.nan\n",
    "            except TypeError:\n",
    "                print('No missing_value on '+name)\n",
    "            try:\n",
    "                dat[name][dat[name] == dat_dict[name].get('_FillValue')*scale_factor] = np.nan\n",
    "            except TypeError:\n",
    "                print('No FillValue on '+name)\n",
    "            add_offset = dat_dict[name].get('add_offset')\n",
    "            if not add_offset:\n",
    "                add_offset = 0\n",
    "            dat[name] = dat[name] + add_offset\n",
    "        except:\n",
    "            print('Problem in filling with nans and getting the offsets, must do it manually')\n",
    "    return dat, dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_field_name(a, name):\n",
    "    names = list(a.dtype.names)\n",
    "    if name in names:\n",
    "        names.remove(name)\n",
    "    b = a[names]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T23:59:10.307391Z",
     "start_time": "2018-08-09T23:59:10.278544Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_netcdf(datfile,values=None,verbose=True,everything=False):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_netcdf\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To compile the functions required to load a netcdf4 file in a similar manner as hdf files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        cdf_data,cdf_dict = load_netcdf(datfile,Values=None,verbose=True) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (netcdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        cdf_data: dictionary with the names of values saved, with associated dictionary values\n",
    "        cdf_dict: metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        values: if ommitted, only outputs the names of the variables in file\n",
    "                needs to be a tuple of 2 element tuples (first: name of variable to be outputted,\n",
    "                second: indes of full name in variables)\n",
    "                example: modis_values=(('tau',35),('lat',22),('lon',23))\n",
    "        verbose: if true (default), then everything is printed. if false, nothing is printed\n",
    "        everything: if true (defaults to false), then everything is saved, without the need for the 'values' variable\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        netcdf4\n",
    "        \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-12-07, NASA Ames\n",
    "        Modified (v1.1): Samuel  LeBlanc, 2018-08-09, NASA Ames Research Center\n",
    "                        Added the 'everything' keyword for loading all the variables from the netcdf file.\n",
    "        \n",
    "    \"\"\"\n",
    "    import netCDF4 as nc\n",
    "    if verbose:\n",
    "        print('Reading file: '+datfile)\n",
    "    f = nc.Dataset(datfile,'r')\n",
    "    varnames = list(f.variables.keys())\n",
    "    if everything: values = [('nul',-999)]\n",
    "    if verbose: \n",
    "        print('Outputting the Data subdatasets:')\n",
    "        for i in range(len(varnames)):\n",
    "            if values:\n",
    "                if everything: values.append((varnames[i].encode('ascii','ignore'),i))\n",
    "                if any(i in val for val in values):\n",
    "                    print('\\x1b[1;36m{0}: {1}\\x1b[0m'.format(i,varnames[i]))\n",
    "                else:\n",
    "                    print('{0}: {1}'.format(i,varnames[i]))\n",
    "            else:\n",
    "                print('{0}: {1}'.format(i,varnames[i]))\n",
    "    if not values:\n",
    "        if verbose:\n",
    "            print('Done going through file... Please supply pairs of name and index for reading file')\n",
    "            print(\" in format values = (('name1',index1),('name2',index2),('name3',index3),...)\")\n",
    "            print(\" where namei is the name of the returned variable, and indexi is the index of the variable (from above)\")\n",
    "        return None, None\n",
    "    if everything: values = values[1:]\n",
    "    cdf_dict = {}\n",
    "    cdf_data = {}\n",
    "    for i,j in values:\n",
    "        cdf_dict[i] = f.variables[varnames[j]]\n",
    "        cdf_data[i] = f.variables[varnames[j]][:]\n",
    "    if verbose:\n",
    "        print(list(cdf_dict.keys()))\n",
    "    return cdf_data,cdf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_aeronet(f,verbose=True,version=2):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_aeronet\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To load the LEV1.0 Aeronet AOD files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aeronet = load_aeronet(f) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        f: path and name of lev10 file\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aeronet: numpy recarray of values\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       verbose: (default True) if True, then prints out info as data is read\n",
    "       version: (default 2) version of the AERONET file (version 3 has a different amount of header)\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        os\n",
    "        load_modis: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        LEV10 file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-09, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import load_utils as lm\n",
    "    import os\n",
    "    if not(os.path.isfile(f)):\n",
    "        raise IOError('Data file {} not found!'.format(f))\n",
    "    if f.split('.')[-1].find('lev10')<0 | f.split('.')[-1].find('LEV10')<0:\n",
    "        raise IOError('Data file {} is not a level 1.0 file - it is not yet available to read'.format(f))\n",
    "    def makeday(txt):\n",
    "        return datetime.strptime(txt,'%d:%m:%Y').timetuple().tm_yday\n",
    "    def maketime(txt):\n",
    "        return lm.toutc(datetime.strptime(txt,'%H:%M:%S'))\n",
    "    conv = {'Dateddmmyy':makeday,'Timehhmmss':maketime}\n",
    "    if verbose:\n",
    "        print('Opening file: {}'.format(f))\n",
    "    if version==3:\n",
    "        header_lines = 6\n",
    "    else:\n",
    "        header_lines = 4\n",
    "    ra = np.genfromtxt(f,skip_header=header_lines,names=True,delimiter=',',converters=conv)\n",
    "    da = lm.recarray_to_dict(ra)\n",
    "    ff = open(f,'r')\n",
    "    lines = ff.readlines()\n",
    "    da['header'] = lines[0:header_lines]\n",
    "    if version==2:\n",
    "        for n in da['header'][2].split(','):\n",
    "            u = n.split('=')\n",
    "            try:\n",
    "                da[u[0]]=float(u[1])\n",
    "            except:\n",
    "                da[u[0]] = u[1].strip()\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_multi_aeronet(dir_path,verbose=True):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_multi_aeronet\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To load multiple files of the LEV1.0 Aeronet AOD files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aeronet = load_multi_aeronet(dir_path) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        dir_path: path of directory where multiple lev10 file reside\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aeronet: numpy recarray of combined values from multiple files\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       verbose: (default True) if True, then prints out info as data is read\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        os\n",
    "        load_utils: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        LEV10 file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-12, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import load_utils as lm\n",
    "    f = os.listdir(dir_path)\n",
    "    aero = []\n",
    "    for fl in f:\n",
    "        aero.append(lm.load_aeronet(dir_path+fl,verbose=verbose))\n",
    "    \n",
    "    n_max = 0\n",
    "    for n in range(len(aero)):\n",
    "        if len(aero[n]['AOT_500'])>n_max: \n",
    "            n_max = len(aero[n]['AOT_500'])\n",
    "            \n",
    "    anet = {}\n",
    "    nstations = len(aero)\n",
    "    for name in list(aero[0].keys()):\n",
    "        if not isinstance(aero[0][name],np.ndarray):\n",
    "            anet[name] = []\n",
    "            for n in range(nstations):\n",
    "                anet[name].append(aero[n][name])\n",
    "        else:\n",
    "            anet[name] = np.zeros((nstations,n_max))+np.nan\n",
    "            for n in range(nstations):\n",
    "                anet[name][n,0:len(aero[n][name])] = aero[n][name]\n",
    "    return anet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aeronet_subset(aero,doy=None,utc=None,julian=None,window=24.0):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        aeronet_subset\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Subsets the aero dict created from load_multi_aeronet for returning \n",
    "        only a index value linking only one point per aeronet station.\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        ii = aeronet_subset(aero,doy=doy,utc=utc,julian=julian,window=24.0) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        aero: aeronet dict of compiled aeronet files from many locations\n",
    "        doy: (default None) put in the day of year value for the time to be selected\n",
    "        utc: (default None) the utc in fractional hours, for the returned aeronet values\n",
    "        julian: (default None) the fractional day of year for the returned values\n",
    "        window: (default 24) the hours of the window, if no value is found within \n",
    "                this window, then returned index links to a nan value\n",
    "        \n",
    "        if there is no doy, utc julian, or window value, \n",
    "        will return the latest value in the dict\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        ii: the index values linking to the searched aero times. \n",
    "            it returns a tuple of indexes to be used directly into the aero dict.\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       see above.\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        none\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-12, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    latest = False\n",
    "    if julian:\n",
    "        in_julian = julian\n",
    "    elif doy:\n",
    "        if utc:\n",
    "            in_julian = doy+utc/24.0\n",
    "        else:\n",
    "            in_julian = doy+0.0\n",
    "    else:\n",
    "        latest = True\n",
    "               \n",
    "    if latest:\n",
    "        ilatest = []\n",
    "        for i,n in enumerate(aero['Location']):\n",
    "            ilatest.append(np.nanargmax(aero['Julian_Day'][i,:]))\n",
    "    else:\n",
    "        ilatest = []\n",
    "        for i,n in enumerate(aero['Location']):\n",
    "            ia = np.nanargmin(abs(aero['Julian_Day'][i,:]-in_julian))\n",
    "            if np.nanmin(abs(aero['Julian_Day'][i,:]-in_julian))*24.0 < window:\n",
    "                ilatest.append(ia)\n",
    "            else:\n",
    "                ia = len(aero['Julian_Day'][i,:])-1\n",
    "                ilatest.append(ia)\n",
    "    iil = []\n",
    "    for i,n in enumerate(ilatest):\n",
    "        iil.append(i)\n",
    "    ii = (iil,ilatest)\n",
    "    \n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recarray_to_dict(ra):\n",
    "    'simple function to convert numpy recarray to a dict with numpy arrays. Useful for modifying the output from genfromtxt'\n",
    "    import numpy as np\n",
    "    da = {}\n",
    "    for n in ra.dtype.names:\n",
    "        da[n]=ra[n]\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_json(f,d):\n",
    "    'Function to save dictionary with numpy elements (d) to a text file (f) define by the JSON typing'\n",
    "    from json_tricks.np import dumps\n",
    "    with open(f,'w') as handle:\n",
    "        handle.write(dumps(d,indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_from_json(f):\n",
    "    'Function to load JSON file and translate to dictionary with numpy elements from text file(f) define by the JSON typing'\n",
    "    from json_tricks.np import loads\n",
    "    with open(f,'r') as handle:\n",
    "        d = dict(loads(handle.read()))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_convert_dict(layer):\n",
    "    import collections\n",
    "    from load_utils import deep_convert_dict\n",
    "    to_ret = layer\n",
    "    if isinstance(layer, collections.OrderedDict):\n",
    "        to_ret = dict(layer)\n",
    "\n",
    "    try:\n",
    "        for key, value in to_ret.items():\n",
    "            to_ret[key] = deep_convert_dict(value)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_convert_dict(layer):\n",
    "    \"\"\"\n",
    "    Function to transform nested ordereddict to nested dict, \n",
    "    \n",
    "    taken from http://stackoverflow.com/questions/25054003/how-to-convert-a-nested-ordereddict-to-dict\n",
    "    \n",
    "    from Patrick Collins\n",
    "    \n",
    "    \"\"\"\n",
    "    import collections\n",
    "    to_ret = layer\n",
    "    if isinstance(layer, collections.OrderedDict):\n",
    "        to_ret = dict(layer)\n",
    "\n",
    "    try:\n",
    "        for key, value in to_ret.items():\n",
    "            to_ret[key] = deep_convert_dict(value)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hdf_raster1(datfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf_raster1\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To load the first raster of the hdf file. \n",
    "        To be used when load_hdf and load_hdf_sd did not work\n",
    "        Brute force load first raster using GDAL\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        dat = load_hdf_raster1(datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile: full path and name of hdf file\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        dat: numpy array of values\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        OSGEO, GDAL\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2019-02-18, Santa Cruz, CA\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    import os\n",
    "    if not(os.path.isfile(datfile)):\n",
    "        raise IOError('Data file {} not found!'.format(datfile))\n",
    "    datsds = gdal.Open(datfile)\n",
    "    rr = datsds.GetRasterBand(1) \n",
    "    g = rr.ReadAsArray()\n",
    "    scale = rr.GetScale()\n",
    "    offset = rr.GetOffset()\n",
    "    nan = rr.GetNoDataValue()\n",
    "    dat = g.astype(float)\n",
    "    dat[dat==nan] = np.nan\n",
    "    dat = dat*scale+offset\n",
    "    del datsds\n",
    "    return dat    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Testing of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "\n",
    "    modis_values = (('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "                    ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "                    ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "                    ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "                    ('cloud_mask',110)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Testing the metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gdal.Open(myd_dat_sub[53][0]).GetMetadata()\n",
    "\n",
    "    mm = dict()\n",
    "    mm['one'] = gdal.Open(myd_dat_sub[72][0]).GetMetadata()\n",
    "    mm['two'] = gdal.Open(myd_dat_sub[74][0]).GetMetadata()\n",
    "    mm['two']['_FillValue']\n",
    "\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    tuple(i[0] for i in modis_values).index('etau')\n",
    "\n",
    "    modis = dict()\n",
    "    modis_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in modis_values:\n",
    "        sds = gdal.Open(myd_dat_sub[j][0])\n",
    "        modis_dicts[i] = sds.GetMetadata()\n",
    "        modis[i] = np.array(sds.ReadAsArray())*float(modis_dicts[i]['scale_factor'])+float(modis_dicts[i]['add_offset'])\n",
    "        modis[i][modis[i] == float(modis_dicts[i]['_FillValue'])] = np.nan\n",
    "        progress(float(tuple(i[0] for i in modis_values).index(i))/len(modis_values)*100.)\n",
    "    endprogress()\n",
    "\n",
    "    print(list(modis.keys()))\n",
    "    print(list(modis_dicts.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
