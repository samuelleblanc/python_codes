{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def __init__():\n",
    "    \"\"\"\n",
    "       Collection of codes to load and analyze various data\n",
    "       \n",
    "           - modis \n",
    "           - emas \n",
    "           - cpl_layers text file\n",
    "           - apr2 files\n",
    "           - hdf files\n",
    "           \n",
    "        details are in the info of each module\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_modis(geofile,datfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_modis\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load Modis files\n",
    "        from within another script\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        modis,modis_dict = load_modis(geofile,datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        geofile name\n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        modis dictionary with tau, ref, etau, eref, phase, qa\n",
    "        modis_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        geo and dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-08, NASA Ames\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    modis_values = (#('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "          #          ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "           #         ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "            #        ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "             #       ('cloud_mask',110)\n",
    "                    )\n",
    "    geosds = gdal.Open(geofile)\n",
    "    datsds = gdal.Open(datfile)\n",
    "    geosub = geosds.GetSubDatasets()\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    print 'Outputting the Geo subdatasets:'\n",
    "    for i in range(len(geosub)):\n",
    "        print str(i)+': '+geosub[i][1]\n",
    "    print 'Outputting the Data subdatasets:'\n",
    "    for i in range(len(datsub)):\n",
    "        if any(i in val for val in modis_values):\n",
    "            print '\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1])\n",
    "        else:\n",
    "            print str(i)+': '+datsub[i][1]\n",
    "    latsds = gdal.Open(geosub[12][0],gdal.GA_ReadOnly)\n",
    "    lonsds = gdal.Open(geosub[13][0],gdal.GA_ReadOnly)\n",
    "    szasds = gdal.Open(geosub[21][0],gdal.GA_ReadOnly)\n",
    "    modis = dict()\n",
    "    modis['lat'] = latsds.ReadAsArray()\n",
    "    modis['lon'] = lonsds.ReadAsArray()\n",
    "    modis['sza'] = szasds.ReadAsArray()\n",
    "    print modis['lon'].shape\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    modis_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in modis_values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        modis_dicts[i] = sds.GetMetadata()\n",
    "        modis[i] = np.array(sds.ReadAsArray())\n",
    "        makenan = True\n",
    "        bad_points = np.where(modis[i] == float(modis_dicts[i]['_FillValue']))\n",
    "        scale = float(modis_dicts[i]['scale_factor'])\n",
    "        offset = float(modis_dicts[i]['add_offset'])\n",
    "       # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "        if scale.is_integer():\n",
    "            scale = int(scale)\n",
    "            makenan = False\n",
    "        if scale != 1 and offset == 0:\n",
    "            modis[i] = modis[i]*scale+offset\n",
    "        if makenan:\n",
    "            modis[i][bad_points] = np.nan\n",
    "        progress(float(tuple(i[0] for i in modis_values).index(i))/len(modis_values)*100.)\n",
    "    endprogress()\n",
    "    print modis.keys()\n",
    "    del geosds, datsds,sds,lonsds,latsds,geosub,datsub\n",
    "    return modis,modis_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_ict(fname,return_header=False,make_nan=True):\n",
    "    \"\"\"\n",
    "    Simple ict file loader\n",
    "    created specifically to load the files from the iwg1 on board the G1 during TCAP, may work with others...\n",
    "    inputs:\n",
    "       fname: filename with full path\n",
    "       return_header: (default set to False) if True, returns data, header in that form\n",
    "       make_nan: (default set to True) if True, the values defined in the header to be missing data, usually -999, is changed to NaNs\n",
    "    \"\"\"\n",
    "    from datetime import datetime\n",
    "    import numpy as np\n",
    "    f = open(fname,'r')\n",
    "    lines = f.readlines()\n",
    "    first = lines[0]\n",
    "    sep = ','\n",
    "    try:\n",
    "        num2skip = int(first.strip().split(sep)[0])\n",
    "    except ValueError:\n",
    "        print 'Seperation is set to a space'\n",
    "        sep = None\n",
    "        num2skip = int(first.strip().split(sep)[0])\n",
    "    header = lines[0:num2skip]\n",
    "    factor = map(float,header[10].strip().split(sep))\n",
    "    missing = map(float,header[11].strip().split(sep))\n",
    "    f.close()\n",
    "    if any([i!=1 for i in factor]):\n",
    "        print('Some Scaling factors are not equal to one, Please check the factors:')\n",
    "        print factor\n",
    "    def mktime(txt):\n",
    "        return datetime.strptime(txt,'%Y-%m-%d %H:%M:%S')\n",
    "    def utctime(seconds_utc):\n",
    "        return float(seconds_utc)/3600.\n",
    "    conv = {\"Date_Time\":mktime, \"UTC\":utctime, \"Start_UTC\":utctime, \"TIME_UTC\":utctime, \"UTC_mid\":utctime}\n",
    "    data = np.genfromtxt(fname,names=True,delimiter=sep,skip_header=num2skip-1,converters=conv)\n",
    "    print data.dtype.names\n",
    "    #scale the values by using the scale factors\n",
    "    for i,name in enumerate(data.dtype.names):\n",
    "        if i>0:\n",
    "            if factor[i-1]!=float(1):\n",
    "                data[name] = data[name]*factor[i-1]\n",
    "            if make_nan:\n",
    "                data[name][data[name]==missing[i-1]]=np.NaN\n",
    "    if return_header:\n",
    "        return data, header\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def modis_qa(qa_array):\n",
    "    \"\"\"\n",
    "    modis qa data parser.\n",
    "    input of qa numpy array\n",
    "    output structure of qa arrays\n",
    "    \"\"\"\n",
    "    bin8 = lambda x : ''.join(reversed( [str((x >> i) & 1) for i in range(8)] ) )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def mat2py_time(matlab_datenum):\n",
    "    \"convert a matlab datenum to a python datetime object. Works on numpy arrays of datenum\"\n",
    "    from datetime import datetime, timedelta\n",
    "    #matlab_datenum = 731965.04835648148\n",
    "    m2ptime = lambda tmat: datetime.fromordinal(int(tmat)) + timedelta(days=tmat%1) - timedelta(days = 366)\n",
    "    try:\n",
    "        python_datetime = m2ptim2(matlab_datenum)\n",
    "    except:\n",
    "        import numpy as np\n",
    "        python_datetime = np.array([m2ptime(matlab_datenum.flatten()[i]) for i in xrange(matlab_datenum.size)])\n",
    "    return python_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def toutc(pydatetime):\n",
    "    \"Convert python datetime to utc fractional hours\"\n",
    "    utc_fx = lambda x: float(x.hour)+float(x.minute)/60.0+float(x.second)/3600.0+float(x.microsecond)/3600000000.0\n",
    "    try: \n",
    "        return utc_fx(pydatetime)\n",
    "    except:\n",
    "        import numpy as np\n",
    "        return np.array([utc_fx(pydatetime.flatten()[i])+(pydatetime.flatten()[i].day-pydatetime.flatten()[0].day)*24.0 \\\n",
    "                         for i in xrange(pydatetime.size)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_emas(datfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_emas\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load emas files\n",
    "        from within another script.\n",
    "        Similar to load_modis\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        emas,emas_dict = load_emas(datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        emas dictionary with tau, ref, etau, eref, phase, qa\n",
    "        emas_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-08, NASA Ames\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    emas_values = (#('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "          #          ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "           #         ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "            #        ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "             #       ('cloud_mask',110)\n",
    "                    )\n",
    "    datsds = gdal.Open(datfile)\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    print 'Outputting the Data subdatasets:'\n",
    "    for i in range(len(datsub)):\n",
    "        if any(i in val for val in emas_values):\n",
    "            print '\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1])\n",
    "        else:\n",
    "            print str(i)+': '+datsub[i][1]\n",
    "    emas = dict()\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    emas_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in emas_values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        emas_dicts[i] = sds.GetMetadata()\n",
    "        emas[i] = np.array(sds.ReadAsArray())\n",
    "        makenan = True\n",
    "        bad_points = np.where(emas[i] == float(emas_dicts[i]['_FillValue']))\n",
    "        try:\n",
    "            scale = float(emas_dicts[i]['scale_factor'])\n",
    "            offset = float(emas_dicts[i]['add_offset'])\n",
    "            # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "            if scale.is_integer():\n",
    "               scale = int(scale)\n",
    "               makenan = False\n",
    "            if scale != 1 and offset == 0:\n",
    "               emas[i] = emas[i]*scale+offset\n",
    "        except:\n",
    "            if issubclass(emas[i].dtype.type, np.integer):\n",
    "                makenan = False\n",
    "        if makenan:\n",
    "            emas[i][bad_points] = np.nan\n",
    "        progress(float(tuple(i[0] for i in emas_values).index(i))/len(emas_values)*100.)\n",
    "    endprogress()\n",
    "    print emas.keys()\n",
    "    del datsds,sds,datsub\n",
    "    return emas,emas_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_hdf(datfile,values=None,verbose=True):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to compile functions required to load emas files\n",
    "        from within another script.\n",
    "        Similar to load_modis\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        hdf_dat,hdf_dict = load_hdf(datfile,Values=None,verbose=True) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (hdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        hdf_dat dictionary with the names of values saved, with associated dictionary values\n",
    "        hdf_dicts : metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        values: if ommitted, only outputs the names of the variables in file\n",
    "                needs to be a tuple of 2 element tuples (first element name of variable, second number of record)\n",
    "                example: modis_values=(('cloud_top',57),('phase',53),('cloud_top_temp',58),('ref',66),('tau',72))\n",
    "        verbose: if true (default), then everything is printed. if false, nothing is printed\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "        Sp_parameters for progress issuer\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2014-12-10, NASA Ames\n",
    "        Modified (v1.1): Samuel LeBlanc, 2015-04-10, NASA Ames\n",
    "                        - added verbose keyword\n",
    "        Modified (v1.2): Samuel LeBlanc, 2016-05-07, Osan AFB, Korea\n",
    "                        - added error handling for missing fill value\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    \n",
    "    datsds = gdal.Open(datfile)\n",
    "    datsub = datsds.GetSubDatasets()\n",
    "    if verbose: \n",
    "        print 'Outputting the Data subdatasets:'\n",
    "        for i in range(len(datsub)):\n",
    "            if values:\n",
    "                if any(i in val for val in values):\n",
    "                    print '\\x1b[1;36m%i: %s\\x1b[0m' %(i,datsub[i][1])\n",
    "                else:\n",
    "                    print str(i)+': '+datsub[i][1]\n",
    "            else:\n",
    "                print str(i)+': '+datsub[i][1]\n",
    "    if not values:\n",
    "        if verbose:\n",
    "            print 'Done going through file... Please supply pairs of name and index for reading file'\n",
    "            print \" in format values = (('name1',index1),('name2',index2),('name3',index3),...)\"\n",
    "            print \" where namei is the nameof the returned variable, and indexi is the index of the variable (from above)\"\n",
    "        return None, None\n",
    "    hdf = dict()\n",
    "    meta = datsds.GetMetadata() \n",
    "    import gc; gc.collect()\n",
    "    hdf_dicts = dict()\n",
    "    if verbose:\n",
    "        startprogress('Running through data values')\n",
    "    for i,j in values:\n",
    "        sds = gdal.Open(datsub[j][0])\n",
    "        hdf_dicts[i] = sds.GetMetadata()\n",
    "        hdf[i] = np.array(sds.ReadAsArray())\n",
    "        if not hdf[i].any():\n",
    "            import pdb; pdb.set_trace()\n",
    "        try:\n",
    "            bad_points = np.where(hdf[i] == float(hdf_dicts[i]['_FillValue']))\n",
    "            makenan = True\n",
    "        except KeyError:\n",
    "            makenan = False\n",
    "        except ValueError:\n",
    "            makenan = False\n",
    "            print '*** FillValue not used to replace NANs, will have to do manually ***'\n",
    "        try:\n",
    "            scale = float(hdf_dicts[i]['scale_factor'])\n",
    "            offset = float(hdf_dicts[i]['add_offset'])\n",
    "            # print 'MODIS array: %s, type: %s' % (i, modis[i].dtype)\n",
    "            if scale.is_integer():\n",
    "               scale = int(scale)\n",
    "               makenan = False\n",
    "            if scale != 1 and offset == 0:\n",
    "               hdf[i] = hdf[i]*scale+offset\n",
    "        except:\n",
    "            if issubclass(hdf[i].dtype.type, np.integer):\n",
    "                makenan = False\n",
    "        if makenan:\n",
    "            hdf[i][bad_points] = np.nan\n",
    "        if verbose:\n",
    "            progress(float(tuple(i[0] for i in values).index(i))/len(values)*100.)\n",
    "    if verbose:\n",
    "        endprogress()\n",
    "        print hdf.keys()\n",
    "    del datsds,sds,datsub\n",
    "    return hdf,hdf_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_cpl_layers(datfile,values=None):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_cpl_layers\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Function to load cpl files of layer properties 'layers_'\n",
    "        This funciton is called from another script\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        cpl_layers = load_cpl_layers(datfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (layers text files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        cpl_layers: dictionary with associated properties such as time, A/C altitude, latitude, longitude, and roll\n",
    "                    number of layers, Ground height (GH) in meters above MSL, top and bottom altitude of each layer, and type of layer\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        os\n",
    "        numpy\n",
    "        re : for regular experessions\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        layers text files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-03-24, NASA Ames\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not(os.path.isfile(datfile)):\n",
    "        error('File not found!')\n",
    "    import numpy as np\n",
    "    import re\n",
    "\n",
    "    # set variables to read\n",
    "    num_lines = sum(1 for line in open(datfile))\n",
    "    head_lines = 14\n",
    "    d = np.empty(num_lines-head_lines,dtype=[('hh','i4'),\n",
    "                                             ('mm','i4'),\n",
    "                                             ('ss','i4'),\n",
    "                                             ('lat','f8'),\n",
    "                                             ('lon','f8'),\n",
    "                                             ('alt','f8'),\n",
    "                                             ('rol','f8'),\n",
    "                                             ('num','i4'),\n",
    "                                             ('gh','f8'),\n",
    "                                             ('top','f8',(10)),\n",
    "                                             ('bot','f8',(10)),\n",
    "                                             ('type','i4',(10)),\n",
    "                                             ('utc','f8')])\n",
    "    d[:] = np.NAN\n",
    "    header = ''\n",
    "    with open(datfile) as fp:\n",
    "        for iline, line in enumerate(fp):\n",
    "            if iline<head_lines:\n",
    "                header = header + line\n",
    "            else:\n",
    "                i = iline-head_lines\n",
    "                line = line.strip()\n",
    "                temp = filter(None, re.split(r\"[ :()]\",line))\n",
    "                d['hh'][i], d['mm'][i], d['ss'][i] = temp[0], temp[1], temp[2]\n",
    "                d['lat'][i], d['lon'][i], d['alt'][i] = temp[3], temp[4], temp[5]\n",
    "                d['rol'][i], d['num'][i], d['gh'][i] = temp[6], temp[7], temp[8]\n",
    "                for n in range(d['num'][i]):\n",
    "                    try:\n",
    "                        d['top'][i][n], d['bot'][i][n], d['type'][i][n] = temp[9+3*n], temp[9+3*n+1], temp[9+3*n+2]\n",
    "                    except:\n",
    "                        import pdb; pdb.set_trace()\n",
    "                d['utc'][i] = float(d['hh'][i])+float(d['mm'][i])/60.0+float(d['ss'][i])/3600.0\n",
    "                \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-140284f6abe9>, line 84)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-140284f6abe9>\"\u001b[1;36m, line \u001b[1;32m84\u001b[0m\n\u001b[1;33m    catch:\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def load_apr(datfiles):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_apr\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Function to load apr values of zenith dbz from the various files in the datfiles list\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aprout = load_apr(datfiles) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfiles name (list of .h4 files to combine)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aprout: dbz, zenith radar reflectivity\n",
    "                latz, latitude of the zenith reflectivity\n",
    "                lonz, longitude of the \"\n",
    "                altflt, actual altitude in the atmosphere of the radar refl.\n",
    "                utc, time of measurement in utc fractional hours\n",
    "                \n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        os\n",
    "        numpy\n",
    "        load_modis\n",
    "        datetime\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        hdf APR-2 files from SEAC4RS\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-04-10, NASA Ames\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from load_utils import load_hdf\n",
    "    import datetime\n",
    "    \n",
    "    first = True\n",
    "    for f in datfiles:\n",
    "        print 'Running file: ',f\n",
    "        if not(os.path.isfile(f)):\n",
    "            print 'Problem with file:', f\n",
    "            print ' ... Skipping'\n",
    "            continue\n",
    "        \n",
    "        apr_value = (('lat',16),('lon',17),('alt',15),('time',13),('dbz',0),('lat3d',30),('lon3d',31),('alt3d',32),('lat3d_o',24),('lon3d_o',25),('alt3d_o',26),('lat3d_s',27),('lon3d_s',28),('alt3d_s',29))\n",
    "        apr,aprdicts = load_hdf(f,values=apr_value,verbose=False)\n",
    "        # transform the 3d latitudes, longitudes, and altitudes to usable values\n",
    "        apr['latz'] = apr['lat3d']/apr['lat3d_s']+apr['lat3d_o']\n",
    "        apr['lonz'] = apr['lon3d']/apr['lon3d_s']+apr['lon3d_o']\n",
    "        apr['altz'] = apr['alt3d']/apr['alt3d_s']+apr['alt3d_o']\n",
    "        apr['altflt'] = np.copy(apr['altz'])\n",
    "        try:\n",
    "            for z in range(apr['altz'].shape[0]):\n",
    "                apr['altflt'][z,:,:] = apr['altz'][z,:,:]+apr['alt'][z,:]\n",
    "        except IndexError:\n",
    "            try:\n",
    "                print 'swaping axes'\n",
    "                apr['altflt'] = np.swapaxes(apr['altflt'],0,1)\n",
    "                apr['altz'] = np.swapaxes(apr['altz'],0,1)\n",
    "                apr['latz'] = np.swapaxes(apr['latz'],0,1)\n",
    "                apr['lonz'] = np.swapaxes(apr['lonz'],0,1)\n",
    "                apr['dbz'] = np.swapaxes(apr['dbz'],0,1)\n",
    "                for z in range(apr['altz'].shape[0]):\n",
    "                    apr['altflt'][z,:,:] = apr['altz'][z,:,:]+apr['alt'][z,:]\n",
    "            except:\n",
    "                print 'Problem file:',f\n",
    "                print '... Skipping'\n",
    "                continue\n",
    "        except:\n",
    "            print 'Problem with file: ',f\n",
    "            print ' ... Skipping'\n",
    "            continue\n",
    "        izen = apr['altz'][:,0,0].argmax() #get the index of zenith\n",
    "        if first:\n",
    "            aprout = dict()\n",
    "            aprout['dbz'] = apr['dbz'][izen,:,:]\n",
    "            aprout['altflt'] = apr['altz'][izen,:,:]+apr['alt'][izen,:]\n",
    "            aprout['latz'] = apr['latz'][izen,:,:]\n",
    "            aprout['lonz'] = apr['lonz'][izen,:,:]\n",
    "            v = datetime.datetime.utcfromtimestamp(apr['time'][izen,0])\n",
    "            aprout['utc'] = (apr['time'][izen,:]-(datetime.datetime(v.year,v.month,v.day,0,0,0)-datetime.datetime(1970,1,1)).total_seconds())/3600.\n",
    "            first = False\n",
    "        else:\n",
    "            aprout['dbz'] = np.concatenate((aprout['dbz'].T,apr['dbz'][izen,:,:].T)).T\n",
    "            aprout['altflt'] = np.concatenate((aprout['altflt'].T,(apr['altz'][izen,:,:]+apr['alt'][izen,:]).T)).T\n",
    "            aprout['latz'] = np.concatenate((aprout['latz'].T,apr['latz'][izen,:,:].T)).T\n",
    "            aprout['lonz'] = np.concatenate((aprout['lonz'].T,apr['lonz'][izen,:,:].T)).T\n",
    "            v = datetime.datetime.utcfromtimestamp(apr['time'][izen,0])\n",
    "            utc = (apr['time'][izen,:]-(datetime.datetime(v.year,v.month,v.day,0,0,0)-datetime.datetime(1970,1,1)).total_seconds())/3600.\n",
    "            aprout['utc'] = np.concatenate((aprout['utc'].T,utc.T)).T\n",
    "            \n",
    "    print aprout.keys()\n",
    "    print 'Loaded data points: ', aprout['utc'].shape\n",
    "    return aprout        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_amsr(datfile,lonlatfile):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_amsr\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to load amsr data into a sucinct dictionary\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        amsr = load_amsr(datfile,lonlatfile) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile: path and name of hdf file\n",
    "        lonlatfile: path and name of hdf files for lat and lon\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        amsr: dictionary with numpy array of values\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        gdal\n",
    "        numpy\n",
    "        gc: for clearing the garbage\n",
    "        Sp_parameters for progress issuer\n",
    "        pdb: for debugging\n",
    "        load_modis: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat file\n",
    "        lonlat file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-05-04, NASA Ames\n",
    "        \n",
    "    \"\"\"\n",
    "    import os\n",
    "    if not(os.path.isfile(datfile)):\n",
    "        error('Data file not found!')\n",
    "    if not(os.path.isfile(lonlatfile)):\n",
    "        error('Lonlat file not found!')\n",
    "    import numpy as np\n",
    "    from load_utils import load_hdf\n",
    "    from osgeo import gdal\n",
    "    gdat = gdal.Open(datfile)\n",
    "    dat = dict()\n",
    "    dat['nfo'] = gdat.GetMetadata()\n",
    "    dat['ice'] = gdat.GetRasterBand(1).ReadAsArray()\n",
    "    datll,dicll = load_hdf(lonlatfile,values=(('lon',0),('lat',1)),verbose=False)\n",
    "    dat['lat'] = datll['lat']\n",
    "    dat['lon'] = datll['lon']\n",
    "    return dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def load_hdf_sd(FILE_NAME):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_hdf_sd\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        to load everyything in a hdf file using the SD protocol instead of GDAL\n",
    "        makes nans out of Missing_value and _FilValue. Scales the values by the scale_factor\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        dat,dat_dict = load_hdf_sd(FILE_NAME) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        FILE_NAME: path and name of hdf file\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        dat: dictionary with numpy array of values\n",
    "        dat_dict: dictionary with dictionaries of attributes for each read value\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       none\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        pyhdf, SDC, SD\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        dat file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-05-13, NASA Ames\n",
    "        Modified (v1.1): by Samuel LeBlanc, 2015-07-01, NASA Ames, Happy Canada Day!\n",
    "                        - added Fill value keyword selection\n",
    "                        - added scale factor and add offset\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from pyhdf.SD import SD, SDC\n",
    "    print 'Reading file: '+FILE_NAME\n",
    "    hdf = SD(FILE_NAME, SDC.READ)\n",
    "    dat = dict()\n",
    "    dat_dict = dict()\n",
    "    for name in hdf.datasets().keys():\n",
    "        print '  '+name+': %s' % (hdf.datasets()[name],)\n",
    "        dat[name] = hdf.select(name)[:]\n",
    "        dat_dict[name] = hdf.select(name).attributes()\n",
    "        try:\n",
    "            scale_factor = dat_dict[name].get('scale_factor')\n",
    "            if not scale_factor:\n",
    "                scale_factor = 1.0\n",
    "            dat[name] = dat[name]*scale_factor\n",
    "            try:\n",
    "                dat[name][dat[name] == dat_dict[name].get('missing_value')*scale_factor] = np.nan\n",
    "            except TypeError:\n",
    "                print 'No missing_value on '+name\n",
    "            try:\n",
    "                dat[name][dat[name] == dat_dict[name].get('_FillValue')*scale_factor] = np.nan\n",
    "            except TypeError:\n",
    "                print 'No FillValue on '+name\n",
    "            add_offset = dat_dict[name].get('add_offset')\n",
    "            if not add_offset:\n",
    "                add_offset = 0\n",
    "            dat[name] = dat[name] + add_offset\n",
    "        except:\n",
    "            print 'Problem in filling with nans and getting the offsets, must do it manually'\n",
    "    return dat, dat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def remove_field_name(a, name):\n",
    "    names = list(a.dtype.names)\n",
    "    if name in names:\n",
    "        names.remove(name)\n",
    "    b = a[names]\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_netcdf(datfile,values=None,verbose=True):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_netcdf\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To compile the functions required to load a netcdf4 file in a similar manner as hdf files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        cdf_data,cdf_dict = load_netcdf(datfile,Values=None,verbose=True) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        datfile name (netcdf files)\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        cdf_data: dictionary with the names of values saved, with associated dictionary values\n",
    "        cdf_dict: metadate for each of the variables\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "        values: if ommitted, only outputs the names of the variables in file\n",
    "                needs to be a tuple of 2 element tuples (first: name of variable to be outputted,\n",
    "                second: indes of full name in variables)\n",
    "                example: modis_values=(('tau',35),('lat',22),('lon',23))\n",
    "        verbose: if true (default), then everything is printed. if false, nothing is printed\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        netcdf4\n",
    "        \n",
    "    Required files:\n",
    "   \n",
    "        dat files\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2015-12-07, NASA Ames\n",
    "        \n",
    "    \"\"\"\n",
    "    import netCDF4 as nc\n",
    "    if verbose:\n",
    "        print 'Reading file: '+datfile\n",
    "    f = nc.Dataset(datfile,'r')\n",
    "    varnames = f.variables.keys()\n",
    "    \n",
    "    if verbose: \n",
    "        print 'Outputting the Data subdatasets:'\n",
    "        for i in range(len(varnames)):\n",
    "            if values:\n",
    "                if any(i in val for val in values):\n",
    "                    print '\\x1b[1;36m{0}: {1}\\x1b[0m'.format(i,varnames[i])\n",
    "                else:\n",
    "                    print '{0}: {1}'.format(i,varnames[i])\n",
    "            else:\n",
    "                print '{0}: {1}'.format(i,varnames[i])\n",
    "    if not values:\n",
    "        if verbose:\n",
    "            print 'Done going through file... Please supply pairs of name and index for reading file'\n",
    "            print \" in format values = (('name1',index1),('name2',index2),('name3',index3),...)\"\n",
    "            print \" where namei is the name of the returned variable, and indexi is the index of the variable (from above)\"\n",
    "        return None, None\n",
    "    \n",
    "    cdf_dict = {}\n",
    "    cdf_data = {}\n",
    "    for i,j in values:\n",
    "        cdf_dict[i] = f.variables[varnames[j]]\n",
    "        cdf_data[i] = f.variables[varnames[j]][:]\n",
    "    if verbose:\n",
    "        print cdf_dict.keys()\n",
    "    return cdf_data,cdf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_aeronet(f,verbose=True):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_aeronet\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To load the LEV1.0 Aeronet AOD files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aeronet = load_aeronet(f) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        f: path and name of lev10 file\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aeronet: numpy recarray of values\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       verbose: (default True) if True, then prints out info as data is read\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        os\n",
    "        load_modis: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        LEV10 file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-09, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from datetime import datetime\n",
    "    import load_utils as lm\n",
    "    import os\n",
    "    if not(os.path.isfile(f)):\n",
    "        raise IOError('Data file {} not found!'.format(f))\n",
    "    if f.split('.')[-1].find('lev10')<0 | f.split('.')[-1].find('LEV10')<0:\n",
    "        raise IOError('Data file {} is not a level 1.0 file - it is not yet available to read'.foramt(f))\n",
    "    def makeday(txt):\n",
    "        return datetime.strptime(txt,'%d:%m:%Y').timetuple().tm_yday\n",
    "    def maketime(txt):\n",
    "        return lm.toutc(datetime.strptime(txt,'%H:%M:%S'))\n",
    "    conv = {'Dateddmmyy':makeday,'Timehhmmss':maketime}\n",
    "    if verbose:\n",
    "        print 'Opening file: {}'.format(f)\n",
    "    ra = np.genfromtxt(f,skip_header=4,names=True,delimiter=',',converters=conv)\n",
    "    da = lm.recarray_to_dict(ra)\n",
    "    ff = open(f,'r')\n",
    "    lines = ff.readlines()\n",
    "    da['header'] = lines[0:4]\n",
    "    for n in da['header'][2].split(','):\n",
    "        u = n.split('=')\n",
    "        try:\n",
    "            da[u[0]]=float(u[1])\n",
    "        except:\n",
    "            da[u[0]] = u[1].strip()\n",
    "    return da    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_multi_aeronet(dir_path,verbose=True):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        load_multi_aeronet\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        To load multiple files of the LEV1.0 Aeronet AOD files\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        aeronet = load_multi_aeronet(dir_path) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        dir_path: path of directory where multiple lev10 file reside\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        aeronet: numpy recarray of combined values from multiple files\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       verbose: (default True) if True, then prints out info as data is read\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "        os\n",
    "        load_utils: this file\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        LEV10 file\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-12, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import load_utils as lm\n",
    "    f = os.listdir(dir_path)\n",
    "    aero = []\n",
    "    for fl in f:\n",
    "        aero.append(lm.load_aeronet(dir_path+fl,verbose=verbose))\n",
    "    \n",
    "    n_max = 0\n",
    "    for n in range(len(aero)):\n",
    "        if len(aero[n]['AOT_500'])>n_max: \n",
    "            n_max = len(aero[n]['AOT_500'])\n",
    "            \n",
    "    anet = {}\n",
    "    nstations = len(aero)\n",
    "    for name in aero[0].keys():\n",
    "        if not isinstance(aero[0][name],np.ndarray):\n",
    "            anet[name] = []\n",
    "            for n in range(nstations):\n",
    "                anet[name].append(aero[n][name])\n",
    "        else:\n",
    "            anet[name] = np.zeros((nstations,n_max))+np.nan\n",
    "            for n in range(nstations):\n",
    "                anet[name][n,0:len(aero[n][name])] = aero[n][name]\n",
    "    return anet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aeronet_subset(aero,doy=None,utc=None,julian=None,window=24.0):\n",
    "    \"\"\"\n",
    "    Name:\n",
    "\n",
    "        aeronet_subset\n",
    "    \n",
    "    Purpose:\n",
    "\n",
    "        Subsets the aero dict created from load_multi_aeronet for returning \n",
    "        only a index value linking only one point per aeronet station.\n",
    "    \n",
    "    Calling Sequence:\n",
    "\n",
    "        ii = aeronet_subset(aero,doy=doy,utc=utc,julian=julian,window=24.0) \n",
    "    \n",
    "    Input: \n",
    "  \n",
    "        aero: aeronet dict of compiled aeronet files from many locations\n",
    "        doy: (default None) put in the day of year value for the time to be selected\n",
    "        utc: (default None) the utc in fractional hours, for the returned aeronet values\n",
    "        julian: (default None) the fractional day of year for the returned values\n",
    "        window: (default 24) the hours of the window, if no value is found within \n",
    "                this window, then returned index links to a nan value\n",
    "        \n",
    "        if there is no doy, utc julian, or window value, \n",
    "        will return the latest value in the dict\n",
    "    \n",
    "    Output:\n",
    "\n",
    "        ii: the index values linking to the searched aero times. \n",
    "            it returns a tuple of indexes to be used directly into the aero dict.\n",
    "    \n",
    "    Keywords: \n",
    "\n",
    "       see above.\n",
    "    \n",
    "    Dependencies:\n",
    "\n",
    "        numpy\n",
    "    \n",
    "    Required files:\n",
    "   \n",
    "        none\n",
    "    \n",
    "    Example:\n",
    "\n",
    "        ...\n",
    "        \n",
    "    Modification History:\n",
    "    \n",
    "        Written (v1.0): Samuel LeBlanc, 2016-05-12, Osan AB, Korea\n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    latest = False\n",
    "    if julian:\n",
    "        in_julian = julian\n",
    "    elif doy:\n",
    "        if utc:\n",
    "            in_julian = doy+utc/24.0\n",
    "        else:\n",
    "            in_julian = doy+0.0\n",
    "    else:\n",
    "        latest = True\n",
    "               \n",
    "    if latest:\n",
    "        ilatest = []\n",
    "        for i,n in enumerate(aero['Location']):\n",
    "            ilatest.append(np.nanargmax(aero['Julian_Day'][i,:]))\n",
    "    else:\n",
    "        ilatest = []\n",
    "        for i,n in enumerate(aero['Location']):\n",
    "            ia = np.nanargmin(abs(aero['Julian_Day'][i,:]-in_julian))\n",
    "            if np.nanmin(abs(aero['Julian_Day'][i,:]-in_julian))*24.0 < window:\n",
    "                ilatest.append(ia)\n",
    "            else:\n",
    "                ia = len(aero['Julian_Day'][i,:])-1\n",
    "                ilatest.append(ia)\n",
    "    iil = []\n",
    "    for i,n in enumerate(ilatest):\n",
    "        iil.append(i)\n",
    "    ii = (iil,ilatest)\n",
    "    \n",
    "    return ii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def recarray_to_dict(ra):\n",
    "    'simple function to convert numpy recarray to a dict with numpy arrays. Useful for modifying the output from genfromtxt'\n",
    "    import numpy as np\n",
    "    da = {}\n",
    "    for n in ra.dtype.names:\n",
    "        da[n]=ra[n]\n",
    "    return da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Testing of the script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "breakpoint": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from osgeo import gdal\n",
    "\n",
    "    modis_values = (('cloud_top',57),\n",
    "                    ('phase',53),\n",
    "                    ('cloud_top_temp',58),\n",
    "                    ('ref',66),\n",
    "                    ('tau',72),\n",
    "                    ('cwp',82),\n",
    "                    ('eref',90),\n",
    "                    ('etau',93),\n",
    "                    ('ecwp',96),\n",
    "                    ('multi_layer',105),\n",
    "                    ('qa',123),\n",
    "                    ('cloud_mask',110)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {}
   },
   "source": [
    "Testing the metadata dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    gdal.Open(myd_dat_sub[53][0]).GetMetadata()\n",
    "\n",
    "    mm = dict()\n",
    "    mm['one'] = gdal.Open(myd_dat_sub[72][0]).GetMetadata()\n",
    "    mm['two'] = gdal.Open(myd_dat_sub[74][0]).GetMetadata()\n",
    "    mm['two']['_FillValue']\n",
    "\n",
    "    from Sp_parameters import startprogress, progress, endprogress\n",
    "    import gc; gc.collect()\n",
    "\n",
    "    tuple(i[0] for i in modis_values).index('etau')\n",
    "\n",
    "    modis = dict()\n",
    "    modis_dicts = dict()\n",
    "    startprogress('Running through modis values')\n",
    "    for i,j in modis_values:\n",
    "        sds = gdal.Open(myd_dat_sub[j][0])\n",
    "        modis_dicts[i] = sds.GetMetadata()\n",
    "        modis[i] = np.array(sds.ReadAsArray())*float(modis_dicts[i]['scale_factor'])+float(modis_dicts[i]['add_offset'])\n",
    "        modis[i][modis[i] == float(modis_dicts[i]['_FillValue'])] = np.nan\n",
    "        progress(float(tuple(i[0] for i in modis_values).index(i))/len(modis_values)*100.)\n",
    "    endprogress()\n",
    "\n",
    "    print modis.keys()\n",
    "    print modis_dicts.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 4,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
